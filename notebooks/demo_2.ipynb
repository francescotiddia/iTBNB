{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis  of text documents using TbNB\n",
    "\n",
    "This is an example showing how TbNB can be used to classify documents by sentiment using a Bag of Words approach. This demo uses a binary document-term sparse matrix to encode the features and demonstrates the correct procedure to correctly train and utilize a (iterative) Threshold-Based Naive Bayes model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72374e5ceeb930e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50d73e477b1b2b8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from TbNB import TbNB  \n",
    "import numpy as np \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:21.901781Z",
     "start_time": "2025-11-26T20:06:21.894096Z"
    }
   },
   "id": "5729d6cf64f730f5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import data and perform train/test split\n",
    "\n",
    "Here we'll employ a simple sentiment dataset containing various reviews. We split the dataset in training and test data (counting 25k samples each), and according to dependent and independent variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edba0c484dcc3ad3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train = dataset[\"train\"].shuffle(seed=42)\n",
    "test = dataset[\"test\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:24.453985Z",
     "start_time": "2025-11-26T20:06:21.933693Z"
    }
   },
   "id": "40593918f0272fd9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_text = train[\"text\"]\n",
    "y_train = np.array(train[\"label\"])\n",
    "X_test_text = test[\"text\"]\n",
    "y_test = np.array(test[\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:24.996712Z",
     "start_time": "2025-11-26T20:06:24.455948Z"
    }
   },
   "id": "e70c7ded544376d2",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Text Vectorization\n",
    "\n",
    "We leverage scikit-learn's CountVectorizer in order to remove stopwords and to create a BoW matrix signifying word presence/absence within each document. The vectorizer is fitted on training and data and is used to transform both training and test data. As the output indicates, CountVectorizer's output type defaults to a scipy sparse matrix, a format especially fitting for BoW data, which allows for extremely fast computations. However, TbNB also accepts other formats for X, such as numpy.ndarray or pandas dataframe. These formats are converted internally into sparse matrices. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c214e7ed1f867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=False, stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:29.424131Z",
     "start_time": "2025-11-26T20:06:24.997231Z"
    }
   },
   "id": "e2c566f48a5d9291",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "scipy.sparse._csr.csr_matrix"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:29.428600Z",
     "start_time": "2025-11-26T20:06:29.425360Z"
    }
   },
   "id": "7f6714f0dc0b7cad",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialize and train the TbNB Model\n",
    "\n",
    "We instantiate the model with iterative=True, which means calling fit will automatically estimate class priors and employ the iterative optimization algorithm described in Romano, M., Zammarchi, G., & Conversano, C. (2024). The .fit() method returns the fitted model and can be used for predictions using dot notation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c9efd0b7b8a79a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "model = TbNB(iterative=True)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Predicted labels:\", y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:29.630089Z",
     "start_time": "2025-11-26T20:06:29.429231Z"
    }
   },
   "id": "3b9f6c5bedf679fc",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ee7890992eff0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate Performance and post-hoc analysis\n",
    "\n",
    "We can evaluate the modelâ€™s accuracy and other metrics using standard scikit-learn functions, as well as inspect learned attributed using the TbNB class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bffe10aa96806a7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.82      0.83     12500\n",
      "    Positive       0.83      0.85      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "Confusion matrix:\n",
      " [[10306  2194]\n",
      " [ 1901 10599]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:29.655402Z",
     "start_time": "2025-11-26T20:06:29.636213Z"
    }
   },
   "id": "299d542fb9e618b5",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect\n",
    " Once the model is trained, one can simply access learned parameters by calling their name using dot notation "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8d9e1dbebf8055"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision threshold (tau_): -2.6851164191490824\n",
      "Number of features (words): 74538\n",
      "Review scores (lambda_scores_):\n",
      "[-0.36425597 -0.34267913 -0.69306718 -1.09845229 -0.69306718  0.69306718\n",
      " -1.38605435  0.69306718  0.69306718  0.87490845]\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision threshold (tau_):\", model.threshold_)\n",
    "print(\"Number of features (words):\", model.n_features_in_)\n",
    "print(\"Review scores (lambda_scores_):\")\n",
    "print(model.lambda_scores_[:10])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:29.662835Z",
     "start_time": "2025-11-26T20:06:29.657264Z"
    }
   },
   "id": "87602cf79b2cb9a0",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "491186b95392f588"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "  Running Vectorizer: Simple\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "  Running Vectorizer: N_grams\n",
      "==============================\n",
      "\n",
      "\n",
      "=== RISULTATI FINALI ===\n",
      "Vectorizer         Model  Accuracy  F1-score  Train Time (s)  Predict Time (s)\n",
      "   N_grams         iTbNB   0.85932  0.859853        0.759851          0.030522\n",
      "   N_grams MultinomialNB   0.85300  0.846151        0.070803          0.040640\n",
      "   N_grams  ComplementNB   0.85300  0.846151        0.081839          0.058662\n",
      "   N_grams          TbNB   0.85224  0.860835        0.798481          0.030292\n",
      "   N_grams   BernoulliNB   0.81460  0.788886        0.103747          0.144499\n",
      "    Simple         iTbNB   0.83620  0.838097        0.234481          0.006187\n",
      "    Simple          TbNB   0.83584  0.836767        0.119916          0.007392\n",
      "    Simple MultinomialNB   0.83192  0.822895        0.011478          0.016750\n",
      "    Simple  ComplementNB   0.83192  0.822895        0.017665          0.015688\n",
      "    Simple   BernoulliNB   0.81540  0.800312        0.029250          0.028783\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "def benchmark_model(name, clf, X_train, X_test, y_train, y_test, variant):\n",
    "    \"\"\"Esegue un benchmark e salva i risultati globali.\"\"\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    preds = clf.predict(X_test)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"binary\")\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"Vectorizer\": variant,\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Predict Time (s)\": pred_time\n",
    "    })\n",
    "\n",
    "\n",
    "vectorizers = {\n",
    "    \"Simple\": CountVectorizer(binary=False, stop_words=\"english\"),\n",
    "    \"N_grams\": CountVectorizer(binary=False, stop_words=\"english\", ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "for variant, vectorizer in vectorizers.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"  Running Vectorizer: {variant}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "    X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "    benchmark_model(\n",
    "        f\"TbNB\",\n",
    "        TbNB(iterative=False),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "    \n",
    "    benchmark_model(\n",
    "        f\"iTbNB\",\n",
    "        TbNB(iterative=True),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"BernoulliNB\",\n",
    "        BernoulliNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"MultinomialNB\",\n",
    "        MultinomialNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"ComplementNB\",\n",
    "        ComplementNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(by=[\"Vectorizer\", \"Accuracy\"], ascending=[True, False])\n",
    "print(\"\\n\\n=== RISULTATI FINALI ===\")\n",
    "print(df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:06:48.505096Z",
     "start_time": "2025-11-26T20:06:29.665641Z"
    }
   },
   "id": "3cdfeba2b5748282",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42f6250542bb3978"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a25e73d8511a672b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d357f908cbc5eb7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "  Running Vectorizer: Simple\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "  Running Vectorizer: N_grams\n",
      "==============================\n",
      "\n",
      "\n",
      "=== RISULTATI FINALI ===\n",
      "Vectorizer         Model  Accuracy  F1-score  Train Time (s)  Predict Time (s)\n",
      "   N_grams MultinomialNB   0.86612  0.862597        0.023634          0.035228\n",
      "   N_grams  ComplementNB   0.86612  0.862597        0.021204          0.026975\n",
      "   N_grams   BernoulliNB   0.86248  0.857557        0.055611          0.036396\n",
      "   N_grams          TbNB   0.86200  0.856894        0.200127          0.010080\n",
      "   N_grams         iTbNB   0.85876  0.851832        0.244095          0.013560\n",
      "    Simple          TbNB   0.84032  0.842064        0.132292          0.007102\n",
      "    Simple         iTbNB   0.84008  0.842784        0.167935          0.008002\n",
      "    Simple MultinomialNB   0.83520  0.826629        0.022400          0.077565\n",
      "    Simple  ComplementNB   0.83520  0.826629        0.024256          0.031025\n",
      "    Simple   BernoulliNB   0.81868  0.804452        0.102576          0.184468\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from preprocessing.nltk_pipeline import TextPreprocessor\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "\n",
    "results = []\n",
    "\n",
    "def benchmark_model(name, pipeline, X_train, X_test, y_train, y_test, variant):\n",
    "    \"\"\"Esegue un benchmark e salva i risultati globali.\"\"\"\n",
    "\n",
    "    # Train\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # Predict\n",
    "    t0 = time.time()\n",
    "    preds = pipeline.predict(X_test)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"binary\")\n",
    "\n",
    "    results.append({\n",
    "        \"Vectorizer\": variant,\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Predict Time (s)\": pred_time\n",
    "    })\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = TextPreprocessor(\n",
    "    language=\"english\",\n",
    "    remove_html=False,\n",
    "    remove_urls=False,\n",
    "    lower=True,\n",
    "    expand_contr=True,\n",
    "    remove_punct=True,\n",
    "    remove_sw=True,\n",
    "    stem=True\n",
    ")\n",
    "\n",
    "\n",
    "vectorizers = {\n",
    "    \"Simple\": CountVectorizer(binary=False),\n",
    "    \"N_grams\": CountVectorizer(binary=False, ngram_range=(1,2), min_df=3),\n",
    "}\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor(\n",
    "    language=\"english\",\n",
    "    remove_html=False,\n",
    "    remove_urls=False,\n",
    "    lower=True,\n",
    "    expand_contr=True,\n",
    "    remove_punct=True,\n",
    "    remove_sw=True,\n",
    "    stem=False\n",
    ")\n",
    "\n",
    "X_train_clean = preprocessor.fit_transform(X_train_text)\n",
    "X_test_clean  = preprocessor.transform(X_test_text)\n",
    "\n",
    "\n",
    "for variant, vectorizer in vectorizers.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"  Running Vectorizer: {variant}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_clean)\n",
    "    X_test_vec = vectorizer.transform(X_test_clean)\n",
    "\n",
    "    benchmark_model(\"TbNB\", TbNB(iterative=False),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"iTbNB\", TbNB(iterative=True),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"BernoulliNB\", BernoulliNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"MultinomialNB\", MultinomialNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"ComplementNB\", ComplementNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(by=[\"Vectorizer\", \"Accuracy\"], ascending=[True, False])\n",
    "print(\"\\n\\n=== RISULTATI FINALI ===\")\n",
    "print(df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:08:00.627652Z",
     "start_time": "2025-11-26T20:07:43.085478Z"
    }
   },
   "id": "a5ddecb6c6b1c8a7",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "25000"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:08:49.519017Z",
     "start_time": "2025-11-26T20:08:49.501670Z"
    }
   },
   "id": "9eb6ad60e6429f0",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "abb6233f9b331d3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
