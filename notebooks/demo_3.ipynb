{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis  of text documents using TbNB\n",
    "\n",
    "This is an example showing how TbNB can be used to classify documents by sentiment using a Bag of Words approach. This demo uses a binary document-term sparse matrix to encode the features and demonstrates the correct procedure to correctly train and utilize a (iterative) Threshold-Based Naive Bayes model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72374e5ceeb930e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50d73e477b1b2b8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from TbNB import TbNB  \n",
    "import numpy as np \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5729d6cf64f730f5",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import data and perform train/test split\n",
    "\n",
    "Here we'll employ a simple sentiment dataset containing various reviews. We split the dataset in training and test data (counting 25k samples each), and according to dependent and independent variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edba0c484dcc3ad3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42a02658d95d45cfaff8700d85fb32fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "train = dataset[\"train\"].shuffle(seed=42).select(range(200000)) \n",
    "test = dataset[\"test\"]\n",
    "\n",
    "X_train_text = train[\"content\"]\n",
    "y_train = np.array(train[\"label\"])\n",
    "X_test_text = test[\"content\"]\n",
    "y_test = np.array(test[\"label\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:03.891023Z",
     "start_time": "2025-11-26T20:09:53.336405Z"
    }
   },
   "id": "40593918f0272fd9",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Text Vectorization\n",
    "\n",
    "We leverage scikit-learn's CountVectorizer in order to remove stopwords and to create a BoW matrix signifying word presence/absence within each document. The vectorizer is fitted on training and data and is used to transform both training and test data. As the output indicates, CountVectorizer's output type defaults to a scipy sparse matrix, a format especially fitting for BoW data, which allows for extremely fast computations. However, TbNB also accepts other formats for X, such as numpy.ndarray or pandas dataframe. These formats are converted internally into sparse matrices. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c214e7ed1f867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=False, stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:50.482363Z",
     "start_time": "2025-11-26T20:10:03.892169Z"
    }
   },
   "id": "e2c566f48a5d9291",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "scipy.sparse._csr.csr_matrix"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:50.492929Z",
     "start_time": "2025-11-26T20:10:50.486107Z"
    }
   },
   "id": "7f6714f0dc0b7cad",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialize and train the TbNB Model\n",
    "\n",
    "We instantiate the model with iterative=True, which means calling fit will automatically estimate class priors and employ the iterative optimization algorithm described in Romano, M., Zammarchi, G., & Conversano, C. (2024). The .fit() method returns the fitted model and can be used for predictions using dot notation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c9efd0b7b8a79a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "model = TbNB(iterative=True)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Predicted labels:\", y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:51.811565Z",
     "start_time": "2025-11-26T20:10:50.499090Z"
    }
   },
   "id": "3b9f6c5bedf679fc",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ee7890992eff0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate Performance and post-hoc analysis\n",
    "\n",
    "We can evaluate the modelâ€™s accuracy and other metrics using standard scikit-learn functions, as well as inspect learned attributed using the TbNB class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bffe10aa96806a7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.86      0.83    200000\n",
      "    Positive       0.85      0.79      0.82    200000\n",
      "\n",
      "    accuracy                           0.82    400000\n",
      "   macro avg       0.83      0.82      0.82    400000\n",
      "weighted avg       0.83      0.82      0.82    400000\n",
      "\n",
      "Confusion matrix:\n",
      " [[172459  27541]\n",
      " [ 42649 157351]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:51.944782Z",
     "start_time": "2025-11-26T20:10:51.830566Z"
    }
   },
   "id": "299d542fb9e618b5",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect\n",
    " Once the model is trained, one can simply access learned parameters by calling their name using dot notation "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8d9e1dbebf8055"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision threshold (tau_): 0.6318971818258774\n",
      "Number of features (words): 167178\n",
      "Review scores (lambda_scores_):\n",
      "[-0.90055648 -0.00290313  0.6990669  -0.68719737 -0.68719737  0.69907699\n",
      " -0.68719737 -0.68719737 -0.68719737 -1.09265251]\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision threshold (tau_):\", model.threshold_)\n",
    "print(\"Number of features (words):\", model.n_features_in_)\n",
    "print(\"Review scores (lambda_scores_):\")\n",
    "print(model.lambda_scores_[:10])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:10:51.953390Z",
     "start_time": "2025-11-26T20:10:51.946711Z"
    }
   },
   "id": "87602cf79b2cb9a0",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "491186b95392f588"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "  Running Vectorizer: Simple\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "  Running Vectorizer: N_grams\n",
      "==============================\n",
      "\n",
      "\n",
      "=== RISULTATI FINALI ===\n",
      "Vectorizer         Model  Accuracy  F1-score  Train Time (s)  Predict Time (s)\n",
      "   N_grams  ComplementNB  0.853507  0.850691        0.184513          0.221479\n",
      "   N_grams MultinomialNB  0.853495  0.850651        0.177454          0.236389\n",
      "   N_grams          TbNB  0.852685  0.850187        2.161524          0.184067\n",
      "   N_grams         iTbNB  0.851615  0.846856        2.638373          0.227796\n",
      "   N_grams   BernoulliNB  0.847565  0.853112        0.261557          0.345673\n",
      "    Simple   BernoulliNB  0.825523  0.826738        0.079414          0.125324\n",
      "    Simple  ComplementNB  0.825483  0.822505        0.026182          0.058849\n",
      "    Simple MultinomialNB  0.825473  0.822435        0.026290          0.060626\n",
      "    Simple          TbNB  0.825348  0.820287        0.435855          0.027804\n",
      "    Simple         iTbNB  0.824525  0.817637        0.889264          0.057638\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "def benchmark_model(name, clf, X_train, X_test, y_train, y_test, variant):\n",
    "    \"\"\"Esegue un benchmark e salva i risultati globali.\"\"\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    preds = clf.predict(X_test)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"binary\")\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"Vectorizer\": variant,\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Predict Time (s)\": pred_time\n",
    "    })\n",
    "\n",
    "\n",
    "vectorizers = {\n",
    "    \"Simple\": CountVectorizer(binary=False, stop_words=\"english\"),\n",
    "    \"N_grams\": CountVectorizer(binary=False, stop_words=\"english\", ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "for variant, vectorizer in vectorizers.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"  Running Vectorizer: {variant}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "    X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "    benchmark_model(\n",
    "        f\"TbNB\",\n",
    "        TbNB(iterative=False),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "    \n",
    "    benchmark_model(\n",
    "        f\"iTbNB\",\n",
    "        TbNB(iterative=True),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"BernoulliNB\",\n",
    "        BernoulliNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"MultinomialNB\",\n",
    "        MultinomialNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "    benchmark_model(\n",
    "        \"ComplementNB\",\n",
    "        ComplementNB(),\n",
    "        X_train_vec,\n",
    "        X_test_vec,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        variant\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(by=[\"Vectorizer\", \"Accuracy\"], ascending=[True, False])\n",
    "print(\"\\n\\n=== RISULTATI FINALI ===\")\n",
    "print(df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:12:57.453513Z",
     "start_time": "2025-11-26T20:10:51.954342Z"
    }
   },
   "id": "3cdfeba2b5748282",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42f6250542bb3978"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a25e73d8511a672b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d357f908cbc5eb7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "  Running Vectorizer: Simple\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "  Running Vectorizer: N_grams\n",
      "==============================\n",
      "\n",
      "\n",
      "=== RISULTATI FINALI ===\n",
      "Vectorizer         Model  Accuracy  F1-score  Train Time (s)  Predict Time (s)\n",
      "   N_grams  ComplementNB  0.860850  0.861387        0.049492          0.253213\n",
      "   N_grams MultinomialNB  0.860842  0.861359        0.105410          0.144594\n",
      "   N_grams          TbNB  0.858625  0.855103        0.869709          0.130540\n",
      "   N_grams         iTbNB  0.858225  0.854132        1.413474          0.154806\n",
      "   N_grams   BernoulliNB  0.854710  0.858948        0.116639          0.209586\n",
      "    Simple  ComplementNB  0.828322  0.825390        0.028737          0.065646\n",
      "    Simple MultinomialNB  0.828275  0.825272        0.031811          0.069199\n",
      "    Simple          TbNB  0.828202  0.825308        0.440209          0.039290\n",
      "    Simple         iTbNB  0.827965  0.823353        0.953476          0.074540\n",
      "    Simple   BernoulliNB  0.827245  0.828834        0.093275          0.123117\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from preprocessing.nltk_pipeline import TextPreprocessor\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "\n",
    "results = []\n",
    "\n",
    "def benchmark_model(name, pipeline, X_train, X_test, y_train, y_test, variant):\n",
    "    \"\"\"Esegue un benchmark e salva i risultati globali.\"\"\"\n",
    "\n",
    "    # Train\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # Predict\n",
    "    t0 = time.time()\n",
    "    preds = pipeline.predict(X_test)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"binary\")\n",
    "\n",
    "    results.append({\n",
    "        \"Vectorizer\": variant,\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Predict Time (s)\": pred_time\n",
    "    })\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = TextPreprocessor(\n",
    "    language=\"english\",\n",
    "    remove_html=False,\n",
    "    remove_urls=False,\n",
    "    lower=True,\n",
    "    expand_contr=False,\n",
    "    remove_punct=True,\n",
    "    remove_sw=True,\n",
    "    stem=False\n",
    ")\n",
    "\n",
    "\n",
    "vectorizers = {\n",
    "    \"Simple\": CountVectorizer(binary=False),\n",
    "    \"N_grams\": CountVectorizer(binary=False, ngram_range=(1,2), min_df=3),\n",
    "}\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor(\n",
    "    language=\"english\",\n",
    "    remove_html=False,\n",
    "    remove_urls=False,\n",
    "    lower=True,\n",
    "    expand_contr=True,\n",
    "    remove_punct=True,\n",
    "    remove_sw=True,\n",
    "    stem=False\n",
    ")\n",
    "\n",
    "X_train_clean = preprocessor.fit_transform(X_train_text)\n",
    "X_test_clean  = preprocessor.transform(X_test_text)\n",
    "\n",
    "\n",
    "for variant, vectorizer in vectorizers.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"  Running Vectorizer: {variant}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_clean)\n",
    "    X_test_vec = vectorizer.transform(X_test_clean)\n",
    "\n",
    "    benchmark_model(\"TbNB\", TbNB(iterative=False),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"iTbNB\", TbNB(iterative=True),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"BernoulliNB\", BernoulliNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"MultinomialNB\", MultinomialNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "    benchmark_model(\"ComplementNB\", ComplementNB(),\n",
    "                    X_train_vec, X_test_vec, y_train, y_test, variant)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(by=[\"Vectorizer\", \"Accuracy\"], ascending=[True, False])\n",
    "print(\"\\n\\n=== RISULTATI FINALI ===\")\n",
    "print(df.to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-26T20:14:34.931134Z",
     "start_time": "2025-11-26T20:12:57.454803Z"
    }
   },
   "id": "a5ddecb6c6b1c8a7",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
